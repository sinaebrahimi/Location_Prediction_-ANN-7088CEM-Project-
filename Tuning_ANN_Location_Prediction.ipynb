{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1722357494439,"user":{"displayName":"Sina Ebrahimi","userId":"05002495946877465600"},"user_tz":-60},"id":"5YoFhmPJNygG"},"outputs":[],"source":["#@title Step 1: Importing the necessary libraries\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pickle\n","import time\n","import itertools\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEZR6q9tNygH"},"outputs":[],"source":["#@title Step 2: Load preprocessed (sequenced) data; Splitting data into training and testing datasets\n","\n","import pickle\n","\n","# Load sequences and vehicle IDs from a file\n","with open('sequences_vehicle_ids.pkl', 'rb') as f:\n","#with open('sequences_vehicle_ids.pkl', 'rb') as f:\n","    sequences, vehicle_ids = pickle.load(f)\n","\n","# Split data into training and testing\n","train_size = int(0.8 * len(sequences))\n","train_sequences = sequences[:train_size]\n","test_sequences = sequences[train_size:]\n","train_vehicle_ids = vehicle_ids[:train_size]\n","test_vehicle_ids = vehicle_ids[train_size:]\n","\n","class VehicleDataset(Dataset):\n","    def __init__(self, sequences, flatten=False):\n","        self.sequences = sequences\n","        self.flatten = flatten # new\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        seq, target = self.sequences[idx]\n","        seq_tensor = torch.Tensor(seq)\n","        if self.flatten:\n","            seq_tensor = seq_tensor.view(-1)  # Flatten the sequence\n","        return seq_tensor, torch.Tensor(target)\n","\n","# Create DataLoader objects for the full dataset\n","train_dataset_all = VehicleDataset(train_sequences, flatten=False)\n","test_dataset_all = VehicleDataset(test_sequences, flatten=False)\n","\n","train_loader_all = DataLoader(train_dataset_all, batch_size=32, shuffle=True)\n","test_loader_all = DataLoader(test_dataset_all, batch_size=32, shuffle=False)\n","\n","# Split train dataset for hyperparameter tuning\n","tune_size = int(0.2 * len(train_sequences))  # Use 10% of the training data for tuning\n","tune_sequences = train_sequences[:tune_size]\n","tune_vehicle_ids = train_vehicle_ids[:tune_size]\n","\n","tune_dataset = VehicleDataset(tune_sequences, flatten=False)\n","tune_test_size = int(0.2 * len(test_sequences))  # Use 10% of the testing data for tuning\n","tune_test_sequences = test_sequences[:tune_test_size]\n","\n","tune_test_dataset = VehicleDataset(tune_test_sequences, flatten=False)\n","\n","# Create DataLoader objects for tuning set\n","train_loader = DataLoader(tune_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(tune_test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEd5lHd4NygI"},"outputs":[],"source":["#@title Step 3: Defining the methods (architecture of NN layers and activation functions)\n","\n","#### Step 3a: Define the Long Short-Term Memory (LSTM) model\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super(LSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","#### Step 3b: Define the Gated Recurrent Unit (GRU) model\n","\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h0)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","#### Step 3c: Define the generic Recurrent Neural Network (RNN) model\n","\n","class RNNModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super(RNNModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.rnn(x, h0)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","#### Step 3d: Define 1-d CNN (Conv1D) model\n","\n","class Conv1DModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super(Conv1DModel, self).__init__()\n","        self.conv_layers = nn.ModuleList()\n","        self.pool_layers = nn.ModuleList()\n","\n","        # Input layer\n","        self.conv_layers.append(nn.Conv1d(input_size, hidden_size, kernel_size=3, padding=1))\n","        self.pool_layers.append(nn.MaxPool1d(kernel_size=2, stride=1))  # Reduced stride\n","\n","        # Hidden layers\n","        for _ in range(1, num_layers):\n","            self.conv_layers.append(nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1))\n","            self.pool_layers.append(nn.MaxPool1d(kernel_size=2, stride=1))  # Reduced stride\n","\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        x = x.transpose(1, 2)  # Swap dimensions to fit Conv1d input format\n","        for conv, pool in zip(self.conv_layers, self.pool_layers):\n","            x = F.relu(conv(x))\n","            x = pool(x)\n","        x = x.mean(dim=2)  # Global average pooling\n","        x = self.fc(x)\n","        return x\n","    \n","#### Step 3e: Define a simple Multi-layer Perceptron (MLP) model\n","\n","class MLPModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super(MLPModel, self).__init__()\n","        layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n","\n","        for _ in range(1, num_layers):\n","            layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n","\n","        layers.append(nn.Linear(hidden_size, output_size))\n","        self.network = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.network(x.view(x.size(0), -1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpVHm3dfNygI"},"outputs":[],"source":["#@title Step 4: Training method (generic for all baselines)\n","\n","def train_model(model, train_loader, criterion, optimizer, num_epochs=10, device='cuda', early_stopping_rounds=4, min_delta=0.01):\n","    model.to(device)\n","    epoch_times = []\n","    epoch_losses = []\n","\n","    for epoch in range(num_epochs):\n","        start_time = time.time()\n","        model.train()\n","        total_loss = 0\n","        for sequences, targets in train_loader:\n","            sequences, targets = sequences.to(device), targets.to(device)\n","            outputs = model(sequences)\n","            loss = criterion(outputs, targets)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        average_loss = total_loss / len(train_loader)\n","        epoch_time = time.time() - start_time\n","        epoch_times.append(epoch_time)\n","        epoch_losses.append(average_loss)\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.8f}, Time: {epoch_time:.2f}s')\n","\n","        # Early stopping mechanism\n","        if epoch >= early_stopping_rounds:\n","          recent_losses = epoch_losses[-early_stopping_rounds:]\n","          if all(abs(recent_losses[i] - recent_losses[i-1]) < min_delta * recent_losses[i-1] for i in range(1, early_stopping_rounds)):\n","              print(f\"Early stopping at epoch {epoch+1}\")\n","              break\n","\n","    return epoch_times, epoch_losses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ru59GgkdNygI"},"outputs":[],"source":["#@title Step 5: Model Evaluation\n","\n","from sklearn.metrics import mean_absolute_error, r2_score\n","\n","def evaluate_model(model, test_loader, device='cuda'):\n","    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model.to(device)\n","\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","\n","    with torch.no_grad():\n","        for sequences, targets in test_loader:\n","            sequences, targets = sequences.to(device), targets.to(device)\n","            outputs = model(sequences)\n","            predictions.extend(outputs.cpu().numpy())\n","            actuals.extend(targets.cpu().numpy())\n","\n","    predictions = np.array(predictions)\n","    actuals = np.array(actuals)\n","\n","    rmse = np.sqrt(np.mean((predictions - actuals) ** 2))\n","    mae = mean_absolute_error(actuals, predictions)\n","    r2 = r2_score(actuals, predictions)\n","\n","    print(f'RMSE: {rmse:.8f}')\n","    print(f'MAE: {mae:.8f}')\n","    print(f'R2 Score: {r2:.8f}')\n","\n","    return predictions, actuals, rmse, mae, r2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1722350092099,"user":{"displayName":"Sina Ebrahimi","userId":"05002495946877465600"},"user_tz":-60},"id":"XBnZTIffNygI","outputId":"d1bffd51-51ee-4471-9bcf-396535dfce57"},"outputs":[],"source":["import torch\n","\n","print(torch.cuda.is_available())\n","print(torch.cuda.device_count())\n","print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")\n","print(torch.__version__)\n","print(torch.version.cuda)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkMcBYrRw_pV"},"outputs":[],"source":["#@title Step 6: Define a method to run the training and evaluation of the model for different sets of hyperparameters\n","\n","def run_experiment(params, input_size=4, output_size=2, model_type='LSTM'):\n","    hidden_size, num_layers, learning_rate, num_epochs = params\n","\n","    if model_type == 'LSTM':\n","        model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n","        save_path = 'experiment_results_LSTM.csv'\n","    elif model_type == 'GRU':\n","        model = GRUModel(input_size, hidden_size, num_layers, output_size)\n","        save_path = 'experiment_results_GRU.csv'\n","    elif model_type == 'RNN':\n","        model = RNNModel(input_size, hidden_size, num_layers, output_size)\n","        save_path = 'experiment_results_RNN.csv'\n","    elif model_type == 'Conv1D':\n","        model = Conv1DModel(input_size, hidden_size, num_layers, output_size)\n","        save_path = 'experiment_results_Conv1D.csv'\n","    elif model_type == 'MLP':\n","        example_input, _ = next(iter(train_loader))\n","        flattened_input_size = example_input.view(example_input.size(0), -1).size(1)\n","        model = MLPModel(flattened_input_size, hidden_size, num_layers, output_size)\n","        save_path = 'experiment_results_MLP.csv'\n","    else:\n","        raise ValueError(f\"Unsupported model type: {model_type}\")\n","\n","    # Initialize criterion and optimizer\n","    criterion = nn.MSELoss() # mean squared error (MSE) is the criterion for evaluating loss in each epoch\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Using Adam optimizer\n","\n","    # Train the model and measure time\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    # Use the provided train_model function\n","    epoch_times, epoch_losses = train_model(model, train_loader, criterion, optimizer, num_epochs=num_epochs, device=device)\n","\n","    # Evaluate the model\n","    predictions, actuals, rmse, mae, r2 = evaluate_model(model, test_loader, device=device)\n","\n","    # Calculate average epoch time\n","    avg_epoch_time = np.mean(epoch_times)\n","\n","    result = {\n","        'hidden_size': hidden_size,\n","        'num_layers': num_layers,\n","        'learning_rate': learning_rate,\n","        'num_epochs': num_epochs,\n","        'rmse': rmse,\n","        'mae': mae,\n","        'r2': r2,\n","        'avg_epoch_time': avg_epoch_time,\n","        'epoch_losses': epoch_losses\n","    }\n","\n","    results_df = pd.DataFrame([result])\n","    if save_path:\n","        if os.path.exists(save_path):\n","            results_df.to_csv(save_path, mode='a', header=False, index=False)\n","        else:\n","            results_df.to_csv(save_path, index=False)\n","\n","    return result\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lzrxhs7dvEos"},"outputs":[],"source":["#@title Step 7: Define hyperparameter sets\n","\n","# Define the hyperparameter grid (2*3*8*1 = 48 total possibilities)\n","param_grid = {\n","    'hidden_size': [50, 100],\n","    'num_layers': [2, 3, 4],\n","    'learning_rate': [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005],\n","    'num_epochs': [30] \n","}\n","\n","# Create a list of hyperparameter combinations\n","param_combinations = list(itertools.product(\n","    param_grid['hidden_size'],\n","    param_grid['num_layers'],\n","    param_grid['learning_rate'],\n","    param_grid['num_epochs']\n","))\n","\n","def run_specific_experiment(index, input_size=4, output_size=2, model_type='LSTM'): # allowed number for index: 0-47\n","    if index < 0 or index >= len(param_combinations):\n","        raise ValueError(\"Index out of range\")\n","    \n","    params = param_combinations[index]\n","\n","    print('model type: ', model_type)\n","    print('index of experiment / total number of experiments: ', str(index+1), ' / ',str(len(param_combinations)))\n","    print('Hyperparameters set (hidden_size, num_layers, learning_rate, num_epochs): ', params)\n","\n","    result = run_experiment(params, input_size, output_size, model_type)\n","    \n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6347655,"status":"error","timestamp":1722357424823,"user":{"displayName":"Sina Ebrahimi","userId":"05002495946877465600"},"user_tz":-60},"id":"in1tQ6oQv-OV","outputId":"52d288f7-b7a1-4dab-ca46-267489d6e2e3"},"outputs":[],"source":["#@title Step 8: Execute each method (flexible for continuing unfinished experiments)\n","\n","# We have 48 sets of experiments due to the different values for hyperparameters in param_grid (defined in Step 7)\n","for i in range(48):\n","    result = run_specific_experiment(index=i, model_type='LSTM')\n","    print('---------')\n","for i in range(48):\n","    result = run_specific_experiment(index=i, model_type='GRU')\n","    print('---------')\n","for i in range(48):\n","    result = run_specific_experiment(index=i, model_type='RNN')\n","    print('---------')\n","for i in range(48): \n","    result = run_specific_experiment(index=i, model_type='Conv1D')\n","    print('---------')\n","for i in range(48):\n","    result = run_specific_experiment(index=i, model_type='MLP')\n","    print('---------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3991,"status":"ok","timestamp":1722297538592,"user":{"displayName":"Sina Ebrahimi","userId":"05002495946877465600"},"user_tz":-60},"id":"-5kAHQLdNygK","outputId":"5b0084df-d986-4a98-d928-a62cbd33d3ff"},"outputs":[],"source":["#@title Step 9: Visualize Results to evalate different hyperparameters\n","import matplotlib.pyplot as plt\n","\n","save_path = 'experiment_results_LSTM.csv' # change LSTM with other methods to visualize metrics for those methods (Methods: RNN, GRU, LSTM, Conv1D, MLP)\n","results_df = pd.read_csv(save_path)\n","\n","\n","# Plot RMSE for different hyperparameters\n","plt.figure(figsize=(14, 7))\n","for key, grp in results_df.groupby(['hidden_size', 'num_layers']):\n","    plt.plot(grp['learning_rate'], grp['rmse'], label=f\"hidden_size={key[0]}, num_layers={key[1]}\")\n","plt.xlabel('Learning Rate')\n","plt.ylabel('RMSE')\n","plt.xscale('log')\n","plt.title('RMSE for Different Hyperparameter Combinations')\n","plt.legend()\n","plt.show()\n","\n","# Plot MAE for different hyperparameters\n","plt.figure(figsize=(14, 7))\n","for key, grp in results_df.groupby(['hidden_size', 'num_layers']):\n","    plt.plot(grp['learning_rate'], grp['mae'], label=f\"hidden_size={key[0]}, num_layers={key[1]}\")\n","plt.xlabel('Learning Rate')\n","plt.ylabel('MAE')\n","plt.xscale('log')\n","plt.title('MAE for Different Hyperparameter Combinations')\n","plt.legend()\n","plt.show()\n","\n","# Plot R2 Score for different hyperparameters\n","plt.figure(figsize=(14, 7))\n","for key, grp in results_df.groupby(['hidden_size', 'num_layers']):\n","    plt.plot(grp['learning_rate'], grp['r2'], label=f\"hidden_size={key[0]}, num_layers={key[1]}\")\n","plt.xlabel('Learning Rate')\n","plt.ylabel('R2 Score')\n","plt.xscale('log')\n","plt.title('R2 Score for Different Hyperparameter Combinations')\n","plt.legend()\n","plt.show()\n","\n","\n","# Plot loss over epochs for different hyperparameters\n","plt.figure(figsize=(14, 7))\n","for idx, result in results_df.iterrows():\n","    epoch_losses = eval(result['epoch_losses'])  # Convert string representation of list back to list\n","    plt.plot(epoch_losses, label=f\"hidden_size={result['hidden_size']}, num_layers={result['num_layers']}, lr={result['learning_rate']}, epochs={result['num_epochs']}\")\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss over Epochs for Different Hyperparameter Combinations')\n","plt.legend()\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"vscode":{"interpreter":{"hash":"e90dcfc948baaa12a49ebafc9d575bbcab636fdcfa13cc0ab4210634dfc6621d"}}},"nbformat":4,"nbformat_minor":0}
